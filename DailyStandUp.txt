What is not working?
What is the one thing which would bring the greatest improvement?

STEPS
Delete double top 1
DATA DICTIONARY
What to do with cross posts. 
Migrate to Postgres
Put it on docker

WB_comments20210523 add to wb_comments
create a sentiment tally

FEATURE ENGINEERING
UTC to GMT+1
+-

Add check for daily table
Add check for main
wb_comments duplicates

19/06/2021
Corrupted MSC with wallstreets bets. 
At this point continuing this project is futile as none of that will be included into dissertation.

18/06/2021
Per last supervisor meeting, no ARIMA and no sentiment.
Nyt data set is enough and provides an ample space for analysis. 

17/06/2021
pickle file is 2GB and pickle is truncate.
computer restarted countless times. 

16/06/2021
found nyt API.

11/06/2021
Reddit data not enough for dissertation. Will need to find new one.
Fingers crossed that dont fluke dissertation with not having a data set. 
not happy. Going to rethink the life.
And yes, contribution to science apparently is important as well. No pressure a month before submission. 

06/06/2021
Python do start counting at 0.
if (datetime.date.today()-datetime.timedelta(days=i)).weekday()< 5:

05/06/2021
If last working day == last in db:
pass
if yahoo date grt last in db:
webscrape

30/05/2021
Next time need to push data from both computers.
Holiday Edite... and I thought you have it all figured out.

28/05/2021
There will be no Wallstreets bets for this day.
Serverless this until next holiday.

25/05/2021
Daily scrape only failed of missing data.py

24/05/2021
Wallstreetbets goes to database
Wallstreets (beets) bets sentiments and comments added to er.py

20/05/2021
Daily scrape fixed after failing. 
hell yeah. 

19/05/2021
+er.py not done fixing

need to add only one entry per date
chech if 2nd item is text, then +2
based of now() figure out the last date is expected to be full with data

18/05/2021
+er.py failed on the daily price injection.
Yahoo added a blank line with todays prices
Some companies have dividends paid out, 17/05/2021 officially a dividend day.

+if date, symbol in the prices - pass
+if line blank - pass
+prices dropped, select distinct, put back
FLIR merged with another company

WALLSTREET BETS SENTIMENT XXXD

16/05/2021
--16/05/2011 HA! I wish I was web scraping in 2011. 
+Slick charts do not permit web-scraping, manual import for now. Don't think anybody will actually challenge SP500 component movements. 
+added DJI and SP500 table
+web scraped historical prices for SP500 component companies

DIVIDENDS!!!!! Just go away. 
Solved in 10 minutes, me accept this. 
Mean while, struggled with table creation.

13/05/2021
+set up selenium to do old_reddit from https://www.redditsearch.io
+old_reddit transposed, added to the main
+main resorted and added anew. 

old+reddit == main == resorted main

12/05/2021
+old reddit headlines can be found at https://www.redditsearch.io
+beautifulsoup does not work

10/05/2021
+SP500/DJI to daily pipeline

07/05/2021
+SQL to merge main, DJI, SP500

04/05/2021
+20/03/2021 fixed, it should have been 20-03-2021.
+Fixed double entries for 20-04-2021.
+New data frame created, with transposed news
+Sp500 /DJI

22/04/2021
+Created basic for loop.
Need to do automate right click- open new tab for some PDFs. They are not accebible in 

PDF can be downloaded from
https://d1wqtxts1xzle7.cloudfront.net/49511010/A_review_of_stock_market_prediction_with20161010-27090-16d1osj.pdf?1476128902=&response-content-disposition=inline%3B+filename%3DA_review_of_stock_market_prediction_with.pdf&Expires=1619098903&Signature=DxSSgzVi8jDkTdx9qOCzPzI127nzjQlpGlGpJ7s0uphEGVmCNU9NsmsPY1ztvcl9yUKh8vWHqHmDci~5toNNddcxEE1W9pubdAMDUYTthZuSwXQrKZg4YRtpJsHNrf-JOUJ22q8N~lnseqzb3YIwaxY1~jRXbzefygLlsg2HLqLD1SuG2zr9io6On8orSA2TKLfRapt6BB-NqjBDnj2pCggNkFTyeTVfdTtIgKLxO3ZT7EK4bsrYU6VmZfL2qgQEcZ9Qvgc4lQ8fcKsSFtAC6tpS6Pgx4PABBGtjj0dHTA6V-MEqDf5ovFJYBUpQeP6EKuH7yYC6j-idSEqeoxz5dw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA

However, under inspection it is below.

<a href="https://www.academia.edu/download/49511010/A_review_of_stock_market_prediction_with20161010-27090-16d1osj.pdf" data-clk="hl=en&amp;sa=T&amp;oi=gga&amp;ct=gga&amp;cd=0&amp;d=7543128795916916980&amp;ei=AW-BYNLDGPqB6rQPx7Kd6A8" data-clk-atid="9Nilt6yTrmgJ"><span class="gs_ctg2">[PDF]</span> academia.edu</a>

21/04/2021
Submitted search phase.
Google scholar have this cool option with PDF embedding.
This is so cool. 
ML which does ML on ML XD

20/04/2021
Selenium
Did I mentioned that python opens new test Chrome window. My face when it worked for the first time. 

Did actually manage to figure out how to open chromedriver
Still cant write into /usr/bin folder. Must love apple products
----
chrome_path = r'/Users/Edite/Downloads/chromedriver'
driver = webdriver.Chrome(executable_path=chrome_path)
driver.get('https://scholar.google.com')
----

16/04/2021
If .py is run from the MSC parent folder, full path need to be provided.

-----
connection = sqlite3.connect('MSC.db')
connection = sqlite3.connect('MSC/MSC.db')
-----

15/04/2021
Converted automatic table creation into .py.
-----
python MSC/er.py
-----

sqlite3.OperationalError: no such table: top30
Need to get rid of top30 individual run and create ETL for all daily tables. 

08/04/2021
Automatic daily table creation succeeded. 
It went easier than I have anticipated.
Next - put web-scraping in .py and schedule with scheduler.

07/04/2021
Got string working for creating the daily table depending on how many different variables are present in the daily scrape.
Need to fix an actual injection part.
Seems like top posts should be called again.
Did get more working than I have planned this morning. 
No Nils though. 

05/04/2021
2 new fields appeared on the Reddit webscrape.
ETL.
Create daily table, which later goes into main one?
Transfer UTC time to the local time. While we are there.

21/03/2021
+ from credentials import credentials
+ Reddit goes to the GitHub. Now I can be less diligent of not deleting some nugget of pure gold. 

I forgot to webscrape reddit yesterday. Did fall asleep at the middle of the Olsen Banden.
In a process of figuring out how to back-engineer yesterdays data, it was discovered that it is rolling 24 hour window for top. 

reddit.subreddit('worldnews').top(time_filter='day', limit=30)

I can increase the dataset before 09/03/2021. China's anal swabs can be part of the data set after all. Victory for the humanity.

There are duplicates in the data set. Should the votes be agglomerated for the duplicates? 
What about when it is top on two days (as it is rolling 26ish hour window)?
What about time of my webscraping, it is PM. However, market is not open and if I would trade the trade would be implemented next morning/day (assuming they are Speedy Gonzales with my trade). So would not it make more economical sense to do it at something ridiculous as 6AM? Or time which is equivalent for America's 6AM, SP500 is traded in USA.
That reminds me that I still have not read Millionaires morning. 

09/03/2021
The issue was with new kind of the field. Omitted for now. I have link to the original post, if it proves to be useful, can go back and fill in missing. 
Dropped params and using top30 columns as params.

Said to Mr.Amazing that Boris Johnson got Covid (again). He challenged me. 
Old news, nice to know that Boris Johnson getting Covid is 19th most up-voted post EVER on r/worldnews.
So either way yesterdays scrape will not be useful. 

top_posts = reddit.subreddit('worldnews').top(time_filter='day',limit=30)

try:
    eval(post._comments)
except:
    print("This is an error message!")

08/03/2021
... data types
First 30 are webscraped, exported to excel. It is 19:50 and I have 0 energy to figure out which column has the wrong date type. 
First time eval() was of use.

========================================================
 
 vars(post)

 =======================================================

07/03/2021
Started to webscrape reddit

Useful resources
https://www.reddit.com/wiki/api
https://praw.readthedocs.io/en/latest/code_overview/reddit_instance.html

Unfortunately, this will not be in data set.
All you can blame is your own procrastination. 

https://www.reddit.com/r/worldnews/comments/lx7sg0/china_make_covid_anal_swabs_mandatory_for_all/
https://www.thesun.co.uk/news/14226455/china-anal-swabs-covid-mandatory/
