What is not working?
What is the one thing which would bring the greatest improvement?

STEPS
Delete double top 1
DATA DICTIONARY
Fix 20/03/2021 so it will appear as the date.
What to do with cross posts.
Add extra check so it would not let me create nedless new DBs. 

FEATURE ENGINEERING
UTC to GMT+1
Is_weekday
Get data for all of march, should I do it from January?

16/4
If .py is run from the MSC parent folder, full path need to be provided.

-----
connection = sqlite3.connect('MSC.db')
connection = sqlite3.connect('MSC/MSC.db')
-----

15/4
Converted automatic table creation into .py.
-----
python MSC/er.py
-----

sqlite3.OperationalError: no such table: top30
Need to get rid of top30 individual run and create ETL for all daily tables. 

8/4
Automatic daily table creation succeeded. 
It went easier than I have anticipated.
Next - put webscraping in .py and schedule with scheduler.

7/4
Got string working for creating the daily table depending on how many different variables are present in the daily scrape.
Need to fix an actual injection part.
Seems like top posts should be called again.
Did get more working then I have planned this morning. 
No Nils though. 

05/04/2021
2 new fields appeared on the Reddit webscrape.
ETL.
Create daily table, which later goes into main one?
Transfer UTC time to the local time. While we are there.

21/03/2021
+ from credentials import credentials
+ Reddit goes to the GitHub. Now I can be less diligent of not deleting some nugget of pure gold. 

I forgot to webscrape reddit yesterday. Did fall asleep at the middle of the Olsen Banden.
In a process of figuring out how to back-engineer yesterdays data, it was discovered that it is rolling 24 hour window for top. 

reddit.subreddit('worldnews').top(time_filter='day', limit=30)

I can increase the dataset before 09/03/2021. China's anal swabs can be part of the data set after all. Victory for the humanity.

There are duplicates in the data set. Should the votes be agglomerated for the duplicates? 
What about when it is top on two days (as it is rolling 26ish hour window)?
What about time of my webscraping, it is PM. However, market is not open and if I would trade the trade would be implemented next morning/day (assuming they are Speedy Gonzales with my trade). So would not it make more economical sense to do it at something ridiculous as 6AM? Or time which is equivalent for America's 6AM, SP500 is traded in USA.
That reminds me that I still have not read Millionaires morning. 

09/03/2021
The issue was with new kind of the field. Omitted for now. I have link to the original post, if it proves to be useful, can go back and fill in missing. 
Dropped params and using top30 columns as params.

Said to Mr.Amazing that Boris Johnson got Covid (again). He challenged me. 
Old news, nice to know that Boris Johnson getting Covid is 19th most up-voted post EVER on r/worldnews.
So either way yesterdays scrape will not be useful. 

top_posts = reddit.subreddit('worldnews').top(time_filter='day',limit=30)

try:
    eval(post._comments)
except:
    print("This is an error message!")

08/03/2021
... data types
First 30 are webscraped, exported to excel. It is 19:50 and I have 0 energy to figure out which column has the wrong date type. 
First time eval() was of use.

========================================================
 
 vars(post)

 =======================================================

07/03/2021
Started to webscrape reddit

Useful resources
https://www.reddit.com/wiki/api
https://praw.readthedocs.io/en/latest/code_overview/reddit_instance.html

Unfortunately, this will not be in data set.
All you can blame is your own procrastination. 

https://www.reddit.com/r/worldnews/comments/lx7sg0/china_make_covid_anal_swabs_mandatory_for_all/
https://www.thesun.co.uk/news/14226455/china-anal-swabs-covid-mandatory/
